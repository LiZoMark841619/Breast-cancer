{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "from my_python.filefinder import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = helper.stat_display()\n",
    "data = helper.sklearn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of the outcome variable: {len(np.unique(data.target))}.' \n",
    "      f'Possible values: {np.unique(data.target)} Labels: {(data.target_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Observations: {data.data.shape[0]}, features: {data.feature_names.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the transformed dataframe from finder \n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the columns as features and investigate at the same time whether they have multicollinearity or not (high correlation)\n",
    "columns = df.columns.to_list()[:10]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = helper.custom_corr(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the helper file\n",
    "helper.custom_heat(df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making X and y as predictor(df for the first run) and outcome(one D array) variables to the model\n",
    "X = df[features]\n",
    "y = data.target\n",
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.sum(y == 0), np.sum(y == 1)\n",
    "print(f'Malingnant tumors number: {a}, benign tumors number: {b}')\n",
    "print(f'The sample is imbalanced, the benign tumors (true positivity) rate is {b/(b+a):.2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_class_size = min(a, b)\n",
    "# At a maximum, there should be no more than the smallest class size divided by 10 number of features.\n",
    "max_features = min_class_size / 10\n",
    "print(f'Maximum features could be {round(max_features)}, the current number of predictor variables is {len(X.columns)}.'\n",
    "      f'The statement that the sample is big enough is {max_features > X.columns.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a pairplot just for fun\n",
    "helper.custom_pair(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.mean(X['mean radius'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing the X values by initalizing the StandardScaler then fit and transform the dataframe (X) back into a 2D array\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X), type(y), X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, d, e, f = np.sum(y_train == 1), np.sum(y_train == 0), np.sum(y_test == 1), np.sum(y_test == 0)\n",
    "f'The train true positivity rate is {c/(c+d):.2}, the test true positivity rate is {e/(e+f):.2}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a LogisticRegression model, fit the training X and y values and then predict y values with using test x values\n",
    "lrm = LogisticRegression(penalty=None, fit_intercept=True, class_weight='balanced')\n",
    "model = lrm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = np.where(y_pred_proba > threshold, 1, 0)\n",
    "y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "  if y_test[i] == 1 and y_pred[i] == 1:\n",
    "    tp += 1\n",
    "  elif y_test[i] == 0 and y_pred[i] == 0:\n",
    "    tn += 1\n",
    "  elif y_test[i] == 0 and y_pred[i] == 1:\n",
    "    fp += 1\n",
    "  else:\n",
    "    fn +=1\n",
    "print(tp, tn, fp, fn)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.custom_confusion(model, X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "``We can see that how using lower prediction threshold changes the results. If our goal is to minimalize the false negatives and the false positives at the same time in a way that their coefficient will be close to 1 that would be a good idea. With this solution we can avoid to mistreat people with malignant tumor while they have benign (False negatives, left bottom corner) or at least decrease the occurences of that, while controlling the false positives (minimalize the number of patitent who has malignant but we predicted benign) can increase the model precision rate which in this case is important. With this in mind the chosen threshold must be somewhere between 0.2 and 0.3 where the precision rate and the recall rate are the closest to each other.``\n",
    "\n",
    "``This is a malignant-benign breast cancer prediction model based on those predictor features using the LogisticRegression algorithm and showing that the prediction threshold has to be chosen depending on the goals we achieve. If we want to save as many lives as possible we should maximize the precision score (as to minimize the FP ) not the number of False negatives. In this case, we should choose 0.6 as the threshold. If we want to maximalize both the recall and the precision scores at the same time we should choose 0.3 as the prediction threshold.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.custom_roc(model, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_and_k = []\n",
    "accuracies = []\n",
    "\n",
    "for k in range(1, 101):\n",
    "  classifier = KNeighborsClassifier(n_neighbors = k)\n",
    "  classifier.fit(X_train, y_train)\n",
    "  a = classifier.score(X_test, y_test)\n",
    "  scores_and_k.append([a, k])\n",
    "  accuracies.append(a)\n",
    "print(max(scores_and_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = list(range(1, 101))\n",
    "plt.plot(k_list, accuracies)\n",
    "plt.xlabel('Number of \"k\" nearest neighbors')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Breast Cancer Classifier Accuracy')\n",
    "plt.plot(max(scores_and_k)[1], max(scores_and_k)[0], '-*')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=3, ccp_alpha=0.01,criterion='gini')\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "tree.plot_tree(dt, feature_names = features, max_depth=5, class_names = ['malignant', 'benign'], label='all', filled=True, rounded=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
